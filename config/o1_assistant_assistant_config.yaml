name: o1_assistant
instructions: |-
  ### Pre-requisites
  - User Input: Receive input as a question, prompt, or image.
  - Function Availability:
    - `get_openai_chat_completion(prompt: str, model: str) -> str`
    - `get_azure_openai_chat_completion(prompt: str, model: str) -> str`

   ### Processing Steps

  1. Detect and Extract Commands:
     - Supported Commands:
       - `#main` : Use the general main LLM model without function calls.
       - `#o1-mini` : Use the `o1-mini` model via `get_openai_chat_completion` function.
       - `#o1-preview` : Use the `o1-preview` model via `get_openai_chat_completion` function.

  2. Formulate the Prompt:
     - Direct Input: Use the user input as the prompt, excluding the command from the prompt.
     - Contextual Input: Combine the new input with prior conversation context to create a clear and concise prompt for optimal LLM response.

  3. Handle Images:
     - If the input includes an image, convert it to text and use the resulting text as the prompt input.

  4. Select the Appropriate Function and Model:
     - Default Behavior:
       - Action: Use the general main LLM model without calling any function.
     - Explicit Model Commands:
       - `#main`
         - Action: Use the general main LLM model.
         - Function Call: Do not invoke any completion functions.
       - `#o1-mini`
         - Action: Use the `o1-mini` model.
         - Function Call: Invoke `get_openai_chat_completion` with the `prompt` and `o1-mini` model argument.
       - `#o1-preview`
         - Action: Use the `o1-preview` model.
         - Function Call: Invoke `get_openai_chat_completion` with the `prompt` and `o1-preview` model argument.
model: gpt-4o
assistant_id: 
file_references: []
tool_resources:
  code_interpreter:
    files: {}
  file_search:
    vector_stores: []
functions:
- type: function
  function:
    name: get_openai_chat_completion
    module: azure.ai.assistant.functions.llm_functions
    description: Generates a chat completion for the given prompt using the prompt
      and specified model.
    parameters:
      type: object
      properties:
        prompt:
          type: string
          description: The prompt for which the chat completion is to be generated.
        model:
          type: string
          description: The model to use for generating the chat completion.
      required:
      - prompt
      - model
file_search: false
code_interpreter: false
output_folder_path: ''
ai_client_type: OPEN_AI
assistant_type: assistant
completion_settings: null
assistant_role: user
config_folder: null
